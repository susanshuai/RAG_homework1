import pandas as pd
import json
from langchain.schema import Document
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
import pandas as pd
import json
from google.colab import userdata
from langchain.chains import RetrievalQA
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain import hub
from uuid import uuid4
from langchain_pinecone import PineconeVectorStore

pc = Pinecone(userdata.get('pinecone_key'))
index_name = 'book-rag-index'
index = pc.Index(index_name)
index.describe_index_stats()

print("Loading documents")
# Loading documents from local disk

data = pd.read_csv("./data/books_1.Best_Books_Ever.csv")
data_dict = data.to_json(orient='index')
data_dict = json.loads(data_dict)
data_list = list(data_dict.values())

# Convert dictionary values into chunks

documents = []

for d in data_list:
  #print(d.get('description',""))
  if 'rating' in d.keys():
    try:
      doc = Document(
          page_content = d.get('description',""),
          metadata = {'rating':d['rating']},
          source = d.get('title',"")
      )
      documents.append(doc)
    except:
      pass

document_chunks = documents


print("Creating embeddings")

text_field = "text"
embeddings = OpenAIEmbeddings(model='text-embedding-ada-002',api_key = userdata.get('OPENAI_API_KEY'))
vectorstore = PineconeVectorStore(index, embeddings, text_field)


uuids = [str(uuid4()) for _ in range(len(document_chunks))]

vectorstore.add_documents(documents=document_chunks, ids=uuids)

print("Creating chains")
# Define whole chain
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 6})

llm = ChatOpenAI(api_key = userdata.get('OPENAI_API_KEY'))#model_name='gpt-4o-mini',temperature=0.0,
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
conversation = ConversationalRetrievalChain.from_llm(
    llm, retriever=retriever, memory=memory, verbose=False)

while(True):
    user_input = input("> ")
    if user_input == "exit":
      break
    result = conversation.invoke(user_input)
    print(result["answer"])
